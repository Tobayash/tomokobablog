---
title: "クラスター分析"
author: "tomokoba"
date: '2020-01-01'
categories: ["Statistics"]
tags: ["Statistics", "Analysis", "Exploratory Factor Analysis"]
---

<meta charset="utf-8" />

<style>
  @import url(https://fonts.googleapis.com/earlyaccess/notosansjapanese.css);
  @import url(https://fonts.googleapis.com/css?family=Lato:400,900);
  body {
  font-size: 100%;
  }
</style>




　以下のコードを使う。
　
```{r eval= FALSE, include=TRUE}
clust <- XXX  #分析したいデータを入れる

# 距離の計算
uclust <-  dist(clust)^2   #ユークリッド距離の平方

result <- hclust(uclust, method="ward.D2")　#Ward法

groups <- cutree(result, k=4) #クラスタ数を4つとする
groups <- factor(groups)

#クラスタに名前を付ける
levels(groups)[1]<-"Group1"
levels(groups)[2]<-"Group2"
levels(groups)[3]<-"Group3"
levels(groups)[3]<-"Group4"

table(groups)

clust$groups<-groups
aggregate(.~groups, data=clust, FUN=mean)
```


### クラスタ数の分析

　クラスタ数を選定する方法として、The Calinski-Harabasz indexがある。<br>
　クラスター数ごとに、クラスタ内の距離の2乗の合計値（within sum of squares; WSS）を算出し、その曲線のエルボーを探す。クラスタ数が増加すると、total WSSが減少するのだが、理想的なクラスター数を超えるとWSSの減少割合が落ちるという見込みからエルボー探しをする。<br>
　また、Calinski-Harabasz基準とは、WSSに対するクラスター間の分散（データセットの重心からすべてのクラスターの重心の分散）の割合である。

```{r}
library(psych)
clust<-epi.bfi[,6:10]
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(reshape2)
# Function to calculate squared distance between two vectors.
sqr_edist <- function(x, y) {
  sum((x-y)^2)
}

# Function to calculate the WSS for a single cluster, which is represented as a matrix (one row for every point)
wss.cluster <- function(clustermat) {
  # Calculate the centroid of the cluster (the mean of all the points).
  c0 <- apply(clustermat, 2, FUN=mean)
  
  # Caculate the squared difference of every point in the cluster from the entroid, and sum all the distances.
  sum(apply(clustermat, 1, FUN=function(row){sqr_edist(row, c0)}))
}

# Function to compute the total WSS from a set of data points and cluster labels.
wss.total <- function(dmatrix, labels) {
  wsstot <- 0
  k <- length(unique(labels))
  for(i in 1:k){
    # Extract each cluster, calculate the cluster's WSS, and sum all the values.
    wsstot <- wsstot + wss.cluster(subset(dmatrix, labels==i))
  }
  wsstot
}

# Listing 8.8 The Calinski-Harabasz index
#
# Convenience function to calculate the total sum of squares.
totss <- function(dmatrix) {
  grandmean <- apply(dmatrix, 2, FUN=mean) # 2 for columns
  sum(apply(dmatrix, 1,
            FUN=function(row){sqr_edist(row, grandmean)}))
}

# A function to calculate the CH index for a number of clusters from 1 to kmax.
ch_criterion <- function(dmatrix, kmax, method="kmeans") {
  if(!(method %in% c("kmeans","hclust"))) {
    stop("meathod must be one of c('kmeans','hclust')") 
  }
  npts <- dim(dmatrix)[1] # number of rows
  
  # The total sum of squares is independent of the clustering.
  totss <- totss(dmatrix)
  
  wss <- numeric(kmax)
  crit <- numeric(kmax)
  
  # Calculate WSS for k = 1 (which is really just total sum of squares).
  wss[1] <- (npts-1)*sum(apply(dmatrix, 2, var))
  
  # Calculate WSS for k from 2 to kmax.
  for (k in 2:kmax) {
    if(method == "kmeans") { # kmenas() returns the total WSS as one of its outputs.
      clustering <- kmeans(dmatrix, k, nstart=10, iter.max = 100)
      wss[k] <- clustering$tot.withinss
    } else { # For hclust(), calculate total WSS by hand.
      d <- dist(dmatrix, method="euclidean")
      pfit <- hclust(d, method="ward.D")
      labels <- cutree(pfit, k=k)
      wss[k] <- wss.total(dmatrix, labels)
    }
  }
  # Calculate BSS for k from 1 to kmax.
  bss <- totss - wss
  # Normalize BSS by k-1.          
  crit.num <- bss/(0:(kmax-1))
  # Normalize WSS by npts-k. 
  crit.denom <- wss/(npts - 1:kmax)
  
  # Return a vector of CH indices and of WSS for k from 1 to kmax. Also return total sum of squares.
  list(crit=crit.num/crit.denom, wss=wss, totss=totss)
}

# Calculate both criteria for 1-10 clusters.
clustcrit <- ch_criterion(clust, 10, method="hclust")

# Create a data frame with the number of clusters, the CH criterion, and the WSS criterion. We'll scale both the CH and WSS criteria to similar ranges so that we can pot them both on the same graph.
critframe <- data.frame(k=1:10, ch=scale(clustcrit$crit), wss=scale(clustcrit$wss))


# Use the melt() function to put the data frame in a shape suitable for ggplot.
critframe <- melt(critframe, id.vars=c("k"), variable.name="measure", value.name="score")

# Plot it.
ggplot(critframe, aes(x=k,y=score,color=measure)) + 
  geom_point(aes(shape=measure)) + geom_line(aes(linetype=measure))+ 
  scale_x_continuous(breaks=1:10, labels=1:10)
```


　エルボー…４…かな？

### クラスタ分析

```{r include=TRUE, warning=FALSE, message=FALSE}
# 距離の計算
uclust <-  dist(clust)^2   #ユークリッド距離の平方

result <- hclust(uclust, method="ward.D2")　#Ward法

groups <- cutree(result, k=4) #クラスタ数を4つとする
groups <- factor(groups)

#クラスタに名前を付ける
levels(groups)[1]<-"Group1"
levels(groups)[2]<-"Group2"
levels(groups)[3]<-"Group3"
levels(groups)[4]<-"Group4"

table(groups)

clust$groups<-groups
per<-aggregate(.~groups, data=clust, FUN=mean)
per

#図示
library(ggplot2)
library(reshape2)
meltd<-melt(per, id.vars="groups",variable.names="items",value.name="per")

ggplot(meltd,aes(x=groups, y=per, fill=variable))+
  geom_bar(stat="identity",position = "dodge", colour ="black",width=0.6)+
  xlab("Personality")+ylab("score")+
  scale_fill_grey(start =0.1, end=1.0)+
  scale_y_continuous(expand = c(0,0))+
  theme_classic()+
  guides(fill=guide_legend(title=NULL))+
  guides(fill=guide_legend(nrow=7))+
  theme(legend.position="right")+
  theme(legend.text=element_text(10))
```

